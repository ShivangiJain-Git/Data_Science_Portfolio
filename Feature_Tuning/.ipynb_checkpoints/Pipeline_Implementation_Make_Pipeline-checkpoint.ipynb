{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A machine learning pipeline is used to help automate machine learning workflows.\n",
    "\n",
    "It consist of several steps to train a model and continuously improve the accuracy of the model and achieve a \n",
    "successful algorithm.\n",
    "\n",
    "A pipeline consists of a **sequence of components which are a compilation of computations**. Data is sent through these components and is manipulated with the help of computation.\n",
    "\n",
    "A typical machine learning pipeline would consist of the following processes:\n",
    "\n",
    "Data collection\n",
    "\n",
    "Data cleaning\n",
    "\n",
    "Feature extraction (labelling and dimensionality reduction)\n",
    "\n",
    "Model validation\n",
    "\n",
    "Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since it is a data file with no header, we will supply the column names which have been obtained from the above URL \n",
    "# Create a python list of column names called \"names\"\n",
    "\n",
    "colnames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "\n",
    "#Load the file from local directory using pd.read_csv which is a special form of read_table\n",
    "#while reading the data, supply the \"colnames\" list\n",
    "\n",
    "pima_df = pd.read_csv(\"pima-indians-diabetes.data\", names= colnames)\n",
    "\n",
    "\n",
    "array = pima_df.values\n",
    "X = array[:,0:7] # select all rows and first 8 columns which are the attributes\n",
    "Y = array[:,8]   # select all rows and the 8th column which is the classification \"Yes\", \"No\" for diabeties\n",
    "test_size = 0.30 # taking 70:30 training and test set\n",
    "seed = 7  # Random numbmer seeding for reapeatability of the code\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('clf', LogisticRegression())])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it takes a list of tuples as parameter. The last entry is the call to the modelling algorithm\n",
    "pipeline = Pipeline([ ('scaler',StandardScaler()), ('clf', LogisticRegression()) ])\n",
    "\n",
    "# use the pipeline object as you would a regular classifer\n",
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score is 0.7792207792207793\n",
      "Confusion Matrix [[132  15]\n",
      " [ 36  48]]\n",
      "Classification Report               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.90      0.84       147\n",
      "         1.0       0.76      0.57      0.65        84\n",
      "\n",
      "    accuracy                           0.78       231\n",
      "   macro avg       0.77      0.73      0.75       231\n",
      "weighted avg       0.78      0.78      0.77       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_predict = pipeline.predict(X_test)\n",
    "model_score = pipeline.score(X_test, y_test)\n",
    "print('Score is',model_score)\n",
    "print('Confusion Matrix',metrics.confusion_matrix(y_test, y_predict))\n",
    "print('Classification Report',metrics.classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are creating an object of Pipeline and then fitting training data using it instead of using model object directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make_Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the pipeline could be cumbersome. Specifying a name to each stage may not\n",
    "be necessary\n",
    "\n",
    "Alternatively there is a **“make_pipeline()” function** that will create the pipeline and automatically name each step.    We do not need to specify a name.\n",
    "\n",
    "a. from sklearn.pipeline import make_pipeline\n",
    "\n",
    "b. pipe = make_pipeline( MinMaxScaler(), (SVC()))\n",
    "\n",
    "c. print(\" Pipeline steps:\\ n{}\". format( pipe.steps))\n",
    "\n",
    " Here we have not specified any name to the stages. The names will be automatically\n",
    "assigned and are usually lowercase of the class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pipeline steps:\\ n[('minmaxscaler', MinMaxScaler()), ('svc', SVC())]\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline( MinMaxScaler(), (SVC())) \n",
    "print(\" Pipeline steps:\\ n{}\". format( pipe.steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('minmaxscaler', MinMaxScaler()), ('svc', SVC())])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit( X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7532467532467533\n"
     ]
    }
   ],
   "source": [
    "# print(\" Test score: {:.2f}\". format( pipe.score( X_test, y_test)))\n",
    "print(pipe.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
